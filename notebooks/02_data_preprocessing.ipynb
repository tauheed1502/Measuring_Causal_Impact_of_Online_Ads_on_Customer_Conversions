{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Causal Analysis\n",
    "\n",
    "This notebook handles:\n",
    "- Data cleaning and validation\n",
    "- Feature engineering for causal inference\n",
    "- Confounder selection\n",
    "- Data preparation for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.data.data_loader import simulate_ad_data\n",
    "from src.data.preprocessing import preprocess, create_treatment_control_split, balance_check\n",
    "from src.data.feature_engineering import engineer_features, select_confounders\n",
    "\n",
    "# Load raw data\n",
    "df_raw = simulate_ad_data(n_samples=12000, seed=42)\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "\n",
    "# Initial data quality check\n",
    "print(\"\\n=== DATA QUALITY OVERVIEW ===\")\n",
    "print(f\"Missing values: {df_raw.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df_raw.duplicated().sum()}\")\n",
    "print(f\"Treatment distribution: {df_raw['treatment'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df_clean = preprocess(df_raw)\n",
    "\n",
    "print(\"=== CLEANING RESULTS ===\")\n",
    "print(f\"Original shape: {df_raw.shape}\")\n",
    "print(f\"Cleaned shape: {df_clean.shape}\")\n",
    "print(f\"Records removed: {len(df_raw) - len(df_clean)}\")\n",
    "\n",
    "# Check for outliers before/after\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Data Cleaning: Before vs After', fontsize=16)\n",
    "\n",
    "# Age distribution\n",
    "axes[0,0].hist(df_raw['age'], bins=30, alpha=0.7, label='Before', color='red')\n",
    "axes[0,0].hist(df_clean['age'], bins=30, alpha=0.7, label='After', color='blue')\n",
    "axes[0,0].set_title('Age Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Income distribution\n",
    "axes[0,1].hist(df_raw['income'], bins=30, alpha=0.7, label='Before', color='red')\n",
    "axes[0,1].hist(df_clean['income'], bins=30, alpha=0.7, label='After', color='blue')\n",
    "axes[0,1].set_title('Income Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Box plots for outlier detection\n",
    "df_raw[['age', 'income']].boxplot(ax=axes[1,0])\n",
    "axes[1,0].set_title('Before Cleaning')\n",
    "\n",
    "df_clean[['age', 'income']].boxplot(ax=axes[1,1])\n",
    "axes[1,1].set_title('After Cleaning')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features for causal analysis\n",
    "df_features = engineer_features(df_clean)\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING RESULTS ===\")\n",
    "print(f\"Original features: {df_clean.shape[1]}\")\n",
    "print(f\"Engineered features: {df_features.shape[1]}\")\n",
    "print(f\"New features added: {df_features.shape[1] - df_clean.shape[1]}\")\n",
    "\n",
    "# Show new features\n",
    "new_features = [col for col in df_features.columns if col not in df_clean.columns]\n",
    "print(f\"\\nNew features: {new_features}\")\n",
    "\n",
    "# Examine feature distributions\n",
    "if len(new_features) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(new_features[:4]):\n",
    "        if df_features[feature].dtype in ['int64', 'float64']:\n",
    "            df_features[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "            axes[i].set_title(f'{feature} Distribution')\n",
    "        else:\n",
    "            df_features[feature].value_counts().plot(kind='bar', ax=axes[i], alpha=0.7)\n",
    "            axes[i].set_title(f'{feature} Counts')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Confounder Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select confounding variables\n",
    "confounders = select_confounders(df_features)\n",
    "\n",
    "print(\"=== CONFOUNDER SELECTION ===\")\n",
    "print(f\"Total available features: {len(df_features.columns)}\")\n",
    "print(f\"Selected confounders: {len(confounders)}\")\n",
    "print(f\"\\nConfounding variables:\")\n",
    "for i, conf in enumerate(confounders, 1):\n",
    "    print(f\"  {i:2d}. {conf}\")\n",
    "\n",
    "# Analyze confounder importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Feature importance for treatment prediction\n",
    "rf_treatment = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_treatment.fit(df_features[confounders], df_features['treatment'])\n",
    "\n",
    "# Feature importance for outcome prediction\n",
    "rf_outcome = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_outcome.fit(df_features[confounders], df_features['conversion'])\n",
    "\n",
    "# Create importance comparison\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': confounders,\n",
    "    'treatment_importance': rf_treatment.feature_importances_,\n",
    "    'outcome_importance': rf_outcome.feature_importances_\n",
    "})\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Treatment prediction importance\n",
    "top_treatment = importance_df.nlargest(10, 'treatment_importance')\n",
    "axes[0].barh(top_treatment['feature'], top_treatment['treatment_importance'], alpha=0.8, color='red')\n",
    "axes[0].set_title('Top 10 Features for Treatment Prediction')\n",
    "axes[0].set_xlabel('Feature Importance')\n",
    "\n",
    "# Outcome prediction importance\n",
    "top_outcome = importance_df.nlargest(10, 'outcome_importance')\n",
    "axes[1].barh(top_outcome['feature'], top_outcome['outcome_importance'], alpha=0.8, color='blue')\n",
    "axes[1].set_title('Top 10 Features for Outcome Prediction')\n",
    "axes[1].set_xlabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strongest confounders (important for both)\n",
    "importance_df['combined_importance'] = importance_df['treatment_importance'] + importance_df['outcome_importance']\n",
    "top_confounders = importance_df.nlargest(5, 'combined_importance')\n",
    "print(f\"\\nStrongest confounders (affect both treatment and outcome):\")\n",
    "for _, row in top_confounders.iterrows():\n",
    "    print(f\"  {row['feature']}: Treatment={row['treatment_importance']:.3f}, Outcome={row['outcome_importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Balance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial covariate balance\n",
    "balance_stats = balance_check(df_features, 'treatment')\n",
    "\n",
    "print(\"=== COVARIATE BALANCE ASSESSMENT ===\")\n",
    "print(f\"Variables assessed: {len(balance_stats)}\")\n",
    "imbalanced_vars = balance_stats[balance_stats['imbalanced'] == True]\n",
    "print(f\"Imbalanced variables: {len(imbalanced_vars)}\")\n",
    "print(f\"Balance rate: {(len(balance_stats) - len(imbalanced_vars)) / len(balance_stats) * 100:.1f}%\")\n",
    "\n",
    "# Show worst imbalances\n",
    "worst_imbalances = balance_stats.reindex(balance_stats['standardized_mean_diff'].abs().sort_values(ascending=False).index).head(10)\n",
    "print(f\"\\nWorst imbalances (top 10):\")\n",
    "for _, row in worst_imbalances.iterrows():\n",
    "    status = \"⚠️\" if row['imbalanced'] else \"✅\"\n",
    "    print(f\"  {status} {row['variable']}: SMD = {row['standardized_mean_diff']:.3f}\")\n",
    "\n",
    "# Visualize balance\n",
    "plt.figure(figsize=(12, 8))\n",
    "variables = worst_imbalances['variable'][:15]\n",
    "smds = worst_imbalances['standardized_mean_diff'][:15]\n",
    "colors = ['red' if abs(smd) > 0.1 else 'green' for smd in smds]\n",
    "\n",
    "bars = plt.barh(variables, smds, color=colors, alpha=0.7)\n",
    "plt.axvline(x=0.1, color='red', linestyle='--', alpha=0.7, label='Imbalance Threshold (+)')\n",
    "plt.axvline(x=-0.1, color='red', linestyle='--', alpha=0.7, label='Imbalance Threshold (-)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Standardized Mean Difference')\n",
    "plt.title('Covariate Balance Before Matching (Top 15 Variables)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset for causal analysis\n",
    "treatment_group, control_group = create_treatment_control_split(df_features)\n",
    "\n",
    "print(\"=== FINAL DATASET SUMMARY ===\")\n",
    "print(f\"Total observations: {len(df_features):,}\")\n",
    "print(f\"Treatment group: {len(treatment_group):,} ({len(treatment_group)/len(df_features)*100:.1f}%)\")\n",
    "print(f\"Control group: {len(control_group):,} ({len(control_group)/len(df_features)*100:.1f}%)\")\n",
    "print(f\"Total features: {len(df_features.columns)}\")\n",
    "print(f\"Confounding variables: {len(confounders)}\")\n",
    "\n",
    "# Outcome statistics\n",
    "print(f\"\\n=== OUTCOME STATISTICS ===\")\n",
    "print(f\"Overall conversion rate: {df_features['conversion'].mean():.3f}\")\n",
    "print(f\"Treatment conversion rate: {treatment_group['conversion'].mean():.3f}\")\n",
    "print(f\"Control conversion rate: {control_group['conversion'].mean():.3f}\")\n",
    "print(f\"Naive treatment effect: {treatment_group['conversion'].mean() - control_group['conversion'].mean():.4f}\")\n",
    "print(f\"True treatment effect: {df_features['true_effect'].iloc[0]:.4f}\")\n",
    "\n",
    "# Save processed data\n",
    "df_features.to_csv('../data/processed/preprocessed_ad_data.csv', index=False)\n",
    "\n",
    "# Save confounder list\n",
    "pd.Series(confounders).to_csv('../data/processed/confounders.csv', index=False, header=['confounder'])\n",
    "\n",
    "print(\"\\n✅ Preprocessed data saved to '../data/processed/'\")\n",
    "print(\"✅ Ready for causal inference modeling\")\n",
    "\n",
    "# Create summary statistics table\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Variable': ['Total Observations', 'Treatment Group', 'Control Group', 'Features', 'Confounders',\n",
    "                'Conversion Rate (Overall)', 'Conversion Rate (Treatment)', 'Conversion Rate (Control)',\n",
    "                'Naive Effect', 'True Effect'],\n",
    "    'Value': [f\"{len(df_features):,}\", f\"{len(treatment_group):,}\", f\"{len(control_group):,}\",\n",
    "             f\"{len(df_features.columns)}\", f\"{len(confounders)}\",\n",
    "             f\"{df_features['conversion'].mean():.3f}\", f\"{treatment_group['conversion'].mean():.3f}\",\n",
    "             f\"{control_group['conversion'].mean():.3f}\",\n",
    "             f\"{treatment_group['conversion'].mean() - control_group['conversion'].mean():.4f}\",\n",
    "             f\"{df_features['true_effect'].iloc[0]:.4f}\"]\n",
    "})\n",
    "\n",
    "print(\"\\n=== PREPROCESSING SUMMARY ===\")\n",
    "print(summary_stats.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
