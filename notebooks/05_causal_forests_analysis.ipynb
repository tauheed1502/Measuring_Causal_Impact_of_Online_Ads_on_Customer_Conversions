{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Forest Analysis\n",
    "\n",
    "This notebook implements causal forests for:\n",
    "- Heterogeneous treatment effect estimation\n",
    "- Individual-level predictions\n",
    "- Subgroup analysis\n",
    "- Targeting recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.models.causal_forest import fit_causal_forest, analyze_heterogeneous_effects\n",
    "from src.utils.plotting import plot_heterogeneous_effects\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/processed/preprocessed_ad_data.csv')\n",
    "confounders = pd.read_csv('../data/processed/confounders.csv')['confounder'].tolist()\n",
    "\n",
    "print(f\"Dataset: {len(df)} observations\")\n",
    "print(f\"Confounders: {len(confounders)} variables\")\n",
    "print(f\"Treatment prevalence: {df['treatment'].mean():.1%}\")\n",
    "print(f\"Overall conversion rate: {df['conversion'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Standard Causal Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit standard causal forest\n",
    "print(\"Fitting Causal Forest...\")\n",
    "cf_results = fit_causal_forest(df, confounders, model_type='standard')\n",
    "\n",
    "print(\"=== CAUSAL FOREST RESULTS ===\")\n",
    "print(f\"Average Treatment Effect: {cf_results['ate']:.4f} ± {cf_results['ate_se']:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{cf_results['ate_ci_lower']:.4f}, {cf_results['ate_ci_upper']:.4f}]\")\n",
    "print(f\"Treatment Effect Variance: {cf_results['effect_variance']:.6f}\")\n",
    "print(f\"Heterogeneity p-value: {cf_results['heterogeneity_pvalue']:.4f}\")\n",
    "\n",
    "# Extract individual treatment effects\n",
    "treatment_effects = cf_results['treatment_effects']\n",
    "print(f\"\\nIndividual Treatment Effects:\")\n",
    "print(f\"  Min: {treatment_effects.min():.4f}\")\n",
    "print(f\"  25th percentile: {np.percentile(treatment_effects, 25):.4f}\")\n",
    "print(f\"  Median: {np.median(treatment_effects):.4f}\")\n",
    "print(f\"  75th percentile: {np.percentile(treatment_effects, 75):.4f}\")\n",
    "print(f\"  Max: {treatment_effects.max():.4f}\")\n",
    "print(f\"  Standard deviation: {treatment_effects.std():.4f}\")\n",
    "\n",
    "# Heterogeneity assessment\n",
    "heterogeneity_significant = cf_results['heterogeneity_pvalue'] < 0.05\n",
    "print(f\"\\nHeterogeneity Assessment:\")\n",
    "print(f\"  Significant heterogeneity: {'✅ Yes' if heterogeneity_significant else '❌ No'}\")\n",
    "print(f\"  Effect range: {treatment_effects.max() - treatment_effects.min():.4f}\")\n",
    "print(f\"  Coefficient of variation: {treatment_effects.std() / abs(treatment_effects.mean()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Treatment Effect Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize treatment effect distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Causal Forest: Treatment Effect Analysis', fontsize=16)\n",
    "\n",
    "# 1. Distribution of individual treatment effects\n",
    "axes[0,0].hist(treatment_effects, bins=50, alpha=0.7, color='skyblue', density=True)\n",
    "axes[0,0].axvline(cf_results['ate'], color='red', linestyle='--', linewidth=2, label=f'ATE = {cf_results[\"ate\"]:.4f}')\n",
    "axes[0,0].axvline(np.median(treatment_effects), color='green', linestyle='--', linewidth=2, label=f'Median = {np.median(treatment_effects):.4f}')\n",
    "axes[0,0].set_xlabel('Individual Treatment Effect')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "axes[0,0].set_title('Distribution of Individual Treatment Effects')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Treatment effects by treatment status\n",
    "treated_effects = treatment_effects[df['treatment'] == 1]\n",
    "control_effects = treatment_effects[df['treatment'] == 0]\n",
    "\n",
    "axes[0,1].hist(control_effects, bins=30, alpha=0.7, label='Control Group', color='blue', density=True)\n",
    "axes[0,1].hist(treated_effects, bins=30, alpha=0.7, label='Treatment Group', color='red', density=True)\n",
    "axes[0,1].set_xlabel('Predicted Treatment Effect')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Treatment Effects by Actual Treatment Status')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Treatment effect quantiles\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "quantile_values = [np.percentile(treatment_effects, q*100) for q in quantiles]\n",
    "\n",
    "axes[1,0].bar([f'{int(q*100)}%' for q in quantiles], quantile_values, alpha=0.8, color='orange')\n",
    "axes[1,0].set_ylabel('Treatment Effect')\n",
    "axes[1,0].set_title('Treatment Effect Quantiles')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(quantile_values):\n",
    "    axes[1,0].text(i, v + 0.0005, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(treatment_effects, dist=\"norm\", plot=axes[1,1])\n",
    "axes[1,1].set_title('Q-Q Plot: Treatment Effects vs Normal Distribution')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Heterogeneous Effects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze heterogeneous effects by subgroups\n",
    "heterogeneity_analysis = analyze_heterogeneous_effects(df, treatment_effects, confounders, n_groups=4)\n",
    "\n",
    "print(\"=== HETEROGENEOUS EFFECTS ANALYSIS ===\")\n",
    "\n",
    "# Analysis by key demographic variables\n",
    "key_vars = ['age', 'income', 'website_visits', 'past_purchases']\n",
    "for var in key_vars:\n",
    "    if var in heterogeneity_analysis:\n",
    "        print(f\"\\n📊 Treatment Effects by {var.title()} Quartiles:\")\n",
    "        effects_by_var = heterogeneity_analysis[var]\n",
    "        \n",
    "        for quartile in range(len(effects_by_var)):\n",
    "            mean_effect = effects_by_var.iloc[quartile]['mean']\n",
    "            std_effect = effects_by_var.iloc[quartile]['std']\n",
    "            count = effects_by_var.iloc[quartile]['count']\n",
    "            print(f\"  Q{quartile+1}: {mean_effect:.4f} ± {std_effect:.4f} (n={count})\")\n",
    "\n",
    "# High vs Low Responders\n",
    "if 'high_responders' in heterogeneity_analysis:\n",
    "    print(f\"\\n🎯 High Responders Profile (Top 25%):\")\n",
    "    high_profile = heterogeneity_analysis['high_responders']\n",
    "    for var in ['age', 'income', 'website_visits', 'past_purchases']:\n",
    "        if var in high_profile:\n",
    "            print(f\"  Average {var}: {high_profile[var]:.2f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Low Responders Profile (Bottom 25%):\")\n",
    "    low_profile = heterogeneity_analysis['low_responders']\n",
    "    for var in ['age', 'income', 'website_visits', 'past_purchases']:\n",
    "        if var in low_profile:\n",
    "            print(f\"  Average {var}: {low_profile[var]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Subgroup Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed subgroup analysis plots\n",
    "df_with_effects = df.copy()\n",
    "df_with_effects['treatment_effect'] = treatment_effects\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Treatment Effect Heterogeneity Analysis', fontsize=16)\n",
    "\n",
    "# Treatment effects by age groups\n",
    "df_with_effects['age_quartile'] = pd.qcut(df_with_effects['age'], q=4, labels=['Young', 'Medium', 'Mature', 'Senior'])\n",
    "age_effects = df_with_effects.groupby('age_quartile')['treatment_effect'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "bars1 = axes[0,0].bar(age_effects.index, age_effects['mean'], yerr=age_effects['std'], capsize=5, alpha=0.8, color='lightblue')\n",
    "axes[0,0].set_title('Treatment Effects by Age Group')\n",
    "axes[0,0].set_ylabel('Average Treatment Effect')\n",
    "axes[0,0].axhline(y=cf_results['ate'], color='red', linestyle='--', alpha=0.7, label='Overall ATE')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean_val in zip(bars1, age_effects['mean']):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                  f'{mean_val:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Treatment effects by income quartiles\n",
    "df_with_effects['income_quartile'] = pd.qcut(df_with_effects['income'], q=4, labels=['Low', 'Med-Low', 'Med-High', 'High'])\n",
    "income_effects = df_with_effects.groupby('income_quartile')['treatment_effect'].agg(['mean', 'std'])\n",
    "\n",
    "bars2 = axes[0,1].bar(income_effects.index, income_effects['mean'], yerr=income_effects['std'], capsize=5, alpha=0.8, color='lightcoral')\n",
    "axes[0,1].set_title('Treatment Effects by Income Quartile')\n",
    "axes[0,1].set_ylabel('Average Treatment Effect')\n",
    "axes[0,1].axhline(y=cf_results['ate'], color='red', linestyle='--', alpha=0.7, label='Overall ATE')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Treatment effect vs key variables\n",
    "axes[0,2].scatter(df_with_effects['age'], df_with_effects['treatment_effect'], alpha=0.6, c=df_with_effects['income'], cmap='viridis')\n",
    "axes[0,2].set_xlabel('Age')\n",
    "axes[0,2].set_ylabel('Treatment Effect')\n",
    "axes[0,2].set_title('Treatment Effect vs Age (colored by Income)')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Treatment effect by website visits\n",
    "visit_effects = df_with_effects.groupby('website_visits')['treatment_effect'].mean()\n",
    "axes[1,0].plot(visit_effects.index, visit_effects.values, marker='o', linewidth=2, markersize=6, color='green')\n",
    "axes[1,0].set_xlabel('Website Visits')\n",
    "axes[1,0].set_ylabel('Average Treatment Effect')\n",
    "axes[1,0].set_title('Treatment Effect by Website Visits')\n",
    "axes[1,0].axhline(y=cf_results['ate'], color='red', linestyle='--', alpha=0.7, label='Overall ATE')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of effects for high vs low past purchasers\n",
    "high_purchasers = df_with_effects[df_with_effects['past_purchases'] > df_with_effects['past_purchases'].median()]['treatment_effect']\n",
    "low_purchasers = df_with_effects[df_with_effects['past_purchases'] <= df_with_effects['past_purchases'].median()]['treatment_effect']\n",
    "\n",
    "axes[1,1].hist(low_purchasers, bins=30, alpha=0.7, label='Low Past Purchases', color='orange', density=True)\n",
    "axes[1,1].hist(high_purchasers, bins=30, alpha=0.7, label='High Past Purchases', color='purple', density=True)\n",
    "axes[1,1].set_xlabel('Treatment Effect')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].set_title('Effect Distribution by Purchase History')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Targeting opportunity analysis\n",
    "effect_deciles = pd.qcut(treatment_effects, q=10, labels=False)\n",
    "targeting_analysis = []\n",
    "\n",
    "for decile in range(10):\n",
    "    mask = effect_deciles == decile\n",
    "    avg_effect = treatment_effects[mask].mean()\n",
    "    population_pct = mask.sum() / len(mask) * 100\n",
    "    targeting_analysis.append({'decile': decile+1, 'avg_effect': avg_effect, 'population_pct': population_pct})\n",
    "\n",
    "targeting_df = pd.DataFrame(targeting_analysis)\n",
    "\n",
    "bars3 = axes[1,2].bar(targeting_df['decile'], targeting_df['avg_effect'], alpha=0.8, color='gold')\n",
    "axes[1,2].set_xlabel('Treatment Effect Decile')\n",
    "axes[1,2].set_ylabel('Average Treatment Effect')\n",
    "axes[1,2].set_title('Targeting Opportunity by Decile')\n",
    "axes[1,2].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[1,2].axhline(y=cf_results['ate'], color='red', linestyle='--', alpha=0.7, label='Overall ATE')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Targeting Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate targeting recommendations\n",
    "print(\"=== TARGETING RECOMMENDATIONS ===\")\n",
    "\n",
    "# Define segments based on treatment effect distribution\n",
    "high_value_threshold = np.percentile(treatment_effects, 75)\n",
    "medium_value_threshold = np.percentile(treatment_effects, 50)\n",
    "low_value_threshold = np.percentile(treatment_effects, 25)\n",
    "\n",
    "# Segment the population\n",
    "high_value_mask = treatment_effects >= high_value_threshold\n",
    "medium_value_mask = (treatment_effects >= medium_value_threshold) & (treatment_effects < high_value_threshold)\n",
    "low_value_mask = (treatment_effects >= low_value_threshold) & (treatment_effects < medium_value_threshold)\n",
    "very_low_value_mask = treatment_effects < low_value_threshold\n",
    "\n",
    "segments = {\n",
    "    'High Value (Top 25%)': {\n",
    "        'mask': high_value_mask,\n",
    "        'avg_effect': treatment_effects[high_value_mask].mean(),\n",
    "        'population': high_value_mask.sum(),\n",
    "        'recommendation': 'Aggressive targeting with premium creatives'\n",
    "    },\n",
    "    'Medium Value (50-75%)': {\n",
    "        'mask': medium_value_mask,\n",
    "        'avg_effect': treatment_effects[medium_value_mask].mean(),\n",
    "        'population': medium_value_mask.sum(),\n",
    "        'recommendation': 'Standard targeting with optimized frequency'\n",
    "    },\n",
    "    'Low Value (25-50%)': {\n",
    "        'mask': low_value_mask,\n",
    "        'avg_effect': treatment_effects[low_value_mask].mean(),\n",
    "        'population': low_value_mask.sum(),\n",
    "        'recommendation': 'Selective targeting with cost optimization'\n",
    "    },\n",
    "    'Very Low Value (Bottom 25%)': {\n",
    "        'mask': very_low_value_mask,\n",
    "        'avg_effect': treatment_effects[very_low_value_mask].mean(),\n",
    "        'population': very_low_value_mask.sum(),\n",
    "        'recommendation': 'Consider exclusion or minimal spend'\n",
    "    }\n",
    "}\n",
    "\n",
    "for segment_name, segment_data in segments.items():\n",
    "    pct_population = segment_data['population'] / len(treatment_effects) * 100\n",
    "    print(f\"\\n🎯 {segment_name}:\")\n",
    "    print(f\"   Population: {segment_data['population']:,} ({pct_population:.1f}%)\")\n",
    "    print(f\"   Avg Treatment Effect: {segment_data['avg_effect']:.4f}\")\n",
    "    print(f\"   Recommendation: {segment_data['recommendation']}\")\n",
    "\n",
    "# Calculate potential ROI improvement\n",
    "print(f\"\\n💰 POTENTIAL ROI IMPROVEMENT:\")\n",
    "print(f\"Current Strategy (treat everyone):\")\n",
    "current_effect = cf_results['ate']\n",
    "print(f\"   Average effect: {current_effect:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimized Strategy (target top 50%):\")\n",
    "top_50_mask = treatment_effects >= medium_value_threshold\n",
    "optimized_effect = treatment_effects[top_50_mask].mean()\n",
    "optimized_population = top_50_mask.sum() / len(treatment_effects)\n",
    "print(f\"   Average effect on targeted population: {optimized_effect:.4f}\")\n",
    "print(f\"   Population targeted: {optimized_population:.1%}\")\n",
    "print(f\"   Efficiency gain: {(optimized_effect - current_effect) / current_effect * 100:.1f}%\")\n",
    "\n",
    "# Profile high-value segment\n",
    "print(f\"\\n📊 HIGH-VALUE SEGMENT PROFILE:\")\n",
    "high_value_profile = df_with_effects[high_value_mask].describe()\n",
    "overall_profile = df_with_effects.describe()\n",
    "\n",
    "key_vars = ['age', 'income', 'website_visits', 'past_purchases']\n",
    "for var in key_vars:\n",
    "    if var in high_value_profile.columns:\n",
    "        high_val_mean = high_value_profile[var]['mean']\n",
    "        overall_mean = overall_profile[var]['mean']\n",
    "        difference = ((high_val_mean - overall_mean) / overall_mean) * 100\n",
    "        print(f\"   {var.title()}: {high_val_mean:.1f} (vs {overall_mean:.1f} overall, {difference:+.1f}% difference)\")\n",
    "\n",
    "# Save causal forest results\n",
    "cf_results['treatment_effects'] = treatment_effects\n",
    "cf_results['targeting_segments'] = segments\n",
    "\n",
    "import pickle\n",
    "with open('../results/causal_forest_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cf_results, f)\n",
    "\n",
    "print(\"\\n✅ Causal Forest results saved to '../results/causal_forest_results.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}\n```
```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Checks for Causal Estimates\n",
    "\n",
    "This notebook conducts comprehensive robustness checks:\n",
    "- Subset validation\n",
    "- Confounder sensitivity analysis\n",
    "- Sample splitting validation\n",
    "- Method comparison across specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.evaluation.robustness_tests import robustness_checks, subset_robustness, confounder_robustness, sample_splitting_validation\n",
    "from src.models.doubly_robust import doubly_robust_estimation\n",
    "from src.models.propensity_score import propensity_score_matching\n",
    "from src.models.causal_forest import fit_causal_forest\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data and previous results\n",
    "df = pd.read_csv('../data/processed/preprocessed_ad_data.csv')\n",
    "confounders = pd.read_csv('../data/processed/confounders.csv')['confounder'].tolist()\n",
    "\n",
    "# Load baseline results for comparison\n",
    "with open('../results/doubly_robust_results.pkl', 'rb') as f:\n",
    "    baseline_dr_results = pickle.load(f)\n",
    "\n",
    "baseline_ate = baseline_dr_results['ate']\n",
    "print(f\"Baseline ATE (Doubly Robust): {baseline_ate:.4f}\")\n",
    "print(f\"Dataset: {len(df)} observations, {len(confounders)} confounders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Subset Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness across different data subsets\n",
    "print(\"=== SUBSET ROBUSTNESS ANALYSIS ===\")\n",
    "print(\"Testing estimate stability across different data subsets...\\n\")\n",
    "\n",
    "# Define multiple subset criteria\n",
    "subsets = {\n",
    "    'random_80pct': df.sample(frac=0.8, random_state=42),\n",
    "    'random_70pct': df.sample(frac=0.7, random_state=123), \n",
    "    'high_engagement': df[df['website_visits'] > df['website_visits'].median()],\n",
    "    'older_users': df[df['age'] > df['age'].median()],\n",
    "    'higher_income': df[df['income'] > df['income'].median()],\n",
    "    'recent_sample': df.tail(int(len(df) * 0.6))  # Last 60% assuming temporal order\n",
    "}\n",
    "\n",
    "subset_results = {}\n",
    "\n",
    "for subset_name, subset_df in subsets.items():\n",
    "    if len(subset_df) > 500:  # Minimum sample size\n",
    "        try:\n",
    "            dr_result = doubly_robust_estimation(subset_df, confounders)\n",
    "            \n",
    "            subset_results[subset_name] = {\n",
    "                'ate': dr_result['ate'],\n",
    "                'ate_se': dr_result['ate_se'],\n",
    "                'sample_size': len(subset_df),\n",
    "                'deviation_from_baseline': abs(dr_result['ate'] - baseline_ate),\n",
    "                'relative_deviation_pct': abs(dr_result['ate'] - baseline_ate) / baseline_ate * 100\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {subset_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"   ATE: {dr_result['ate']:.4f} ± {dr_result['ate_se']:.4f}\")\n",
    "            print(f\"   Sample size: {len(subset_df):,}\")\n",
    "            print(f\"   Deviation from baseline: {subset_results[subset_name]['deviation_from_baseline']:.4f} ({subset_results[subset_name]['relative_deviation_pct']:.1f}%)\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {subset_name}: Failed - {str(e)[:100]}\\n\")\n",
    "            \n",
    "# Visualize subset results\n",
    "if subset_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ATE estimates across subsets\n",
    "    subset_names = list(subset_results.keys())\n",
    "    ate_estimates = [subset_results[name]['ate'] for name in subset_names]\n",
    "    ate_errors = [subset_results[name]['ate_se'] for name in subset_names]\n",
    "    \n",
    "    bars = axes[0].bar(range(len(subset_names)), ate_estimates, yerr=ate_errors, \n",
    "                      capsize=5, alpha=0.8, color='skyblue')\n",
    "    axes[0].axhline(y=baseline_ate, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Baseline ATE = {baseline_ate:.4f}')\n",
    "    axes[0].set_xticks(range(len(subset_names)))\n",
    "    axes[0].set_xticklabels([name.replace('_', '\\n').title() for name in subset_names], rotation=45)\n",
    "    axes[0].set_ylabel('ATE Estimate')\n",
    "    axes[0].set_title('ATE Estimates Across Subsets')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Deviation from baseline\n",
    "    deviations = [subset_results[name]['relative_deviation_pct'] for name in subset_names]\n",
    "    colors = ['green' if dev < 10 else 'orange' if dev < 20 else 'red' for dev in deviations]\n",
    "    \n",
    "    axes[1].bar(range(len(subset_names)), deviations, alpha=0.8, color=colors)\n",
    "    axes[1].axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='10% threshold')\n",
    "    axes[1].axhline(y=20, color='red', linestyle='--', alpha=0.7, label='20% threshold')\n",
    "    axes[1].set_xticks(range(len(subset_names)))\n",
    "    axes[1].set_xticklabels([name.replace('_', '\\n').title() for name in subset_names], rotation=45)\n",
    "    axes[1].set_ylabel('Deviation from Baseline (%)')\n",
    "    axes[1].set_title('Relative Deviation from Baseline ATE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    max_deviation = max(deviations)\n",
    "    avg_deviation = np.mean(deviations)\n",
    "    robust_subsets = sum([1 for dev in deviations if dev < 15])\n",
    "    \n",
    "    print(f\"\\n📊 SUBSET ROBUSTNESS SUMMARY:\")\n",
    "    print(f\"   Maximum deviation: {max_deviation:.1f}%\")\n",
    "    print(f\"   Average deviation: {avg_deviation:.1f}%\")\n",
    "    print(f\"   Robust subsets (<15% deviation): {robust_subsets}/{len(deviations)}\")\n",
    "    print(f\"   Overall robustness: {'✅ Strong' if avg_deviation < 10 else '⚠️ Moderate' if avg_deviation < 20 else '❌ Weak'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Confounder Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to confounder selection\n",
    "print(\"\\n=== CONFOUNDER SENSITIVITY ANALYSIS ===\")\n",
    "print(\"Testing estimate sensitivity to different confounder specifications...\\n\")\n",
    "\n",
    "confounder_tests = {}\n",
    "\n",
    "# Test 1: Drop least important confounders\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(df[confounders], df['treatment'])\n",
    "\n",
    "# Get feature importance and sort\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': confounders,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Test different confounder sets\n",
    "test_specs = {\n",
    "    'all_confounders': confounders,\n",
    "    'top_80pct': importance_df.head(int(len(confounders) * 0.8))['feature'].tolist(),\n",
    "    'top_60pct': importance_df.head(int(len(confounders) * 0.6))['feature'].tolist(),\n",
    "    'top_10_most_important': importance_df.head(10)['feature'].tolist(),\n",
    "    'exclude_least_important': importance_df.head(len(confounders)-2)['feature'].tolist(),\n",
    "    'core_demographics': [col for col in ['age', 'income', 'website_visits', 'past_purchases'] if col in confounders]\n",
    "}\n",
    "\n",
    "for spec_name, confounder_set in test_specs.items():\n",
    "    if len(confounder_set) > 0:\n",
    "        try:\n",
    "            dr_result = doubly_robust_estimation(df, confounder_set)\n",
    "            \n",
    "            confounder_tests[spec_name] = {\n",
    "                'ate': dr_result['ate'],\n",
    "                'ate_se': dr_result['ate_se'],\n",
    "                'confounders_count': len(confounder_set),\n",
    "                'deviation_from_baseline': abs(dr_result['ate'] - baseline_ate),\n",
    "                'relative_deviation_pct': abs(dr_result['ate'] - baseline_ate) / baseline_ate * 100\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {spec_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"   ATE: {dr_result['ate']:.4f} ± {dr_result['ate_se']:.4f}\")\n",
    "            print(f\"   Confounders used: {len(confounder_set)}\")\n",
    "            print(f\"   Deviation: {confounder_tests[spec_name]['deviation_from_baseline']:.4f} ({confounder_tests[spec_name]['relative_deviation_pct']:.1f}%)\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {spec_name}: Failed - {str(e)[:100]}\\n\")\n",
    "\n",
    "# Visualize confounder sensitivity\n",
    "if confounder_tests:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    spec_names = list(confounder_tests.keys())\n",
    "    ate_estimates = [confounder_tests[name]['ate'] for name in spec_names]\n",
    "    confounder_counts = [confounder_tests[name]['confounders_count'] for name in spec_names]\n",
    "    \n",
    "    # ATE by number of confounders\n",
    "    axes[0].scatter(confounder_counts, ate_estimates, s=100, alpha=0.8, color='purple')\n",
    "    axes[0].axhline(y=baseline_ate, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Baseline ATE = {baseline_ate:.4f}')\n",
    "    axes[0].set_xlabel('Number of Confounders')\n",
    "    axes[0].set_ylabel('ATE Estimate')\n",
    "    axes[0].set_title('ATE Estimates by Confounder Count')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, (x, y, name) in enumerate(zip(confounder_counts, ate_estimates, spec_names)):\n",
    "        axes[0].annotate(name.replace('_', '\\n'), (x, y), xytext=(5, 5), \n",
    "                        textcoords='offset points', fontsize=8, alpha=0.8)\n",
    "    \n",
    "    # Deviation by specification\n",
    "    deviations = [confounder_tests[name]['relative_deviation_pct'] for name in spec_names]\n",
    "    colors = ['green' if dev < 5 else 'orange' if dev < 15 else 'red' for dev in deviations]\n",
    "    \n",
    "    bars = axes[1].bar(range(len(spec_names)), deviations, alpha=0.8, color=colors)\n",
    "    axes[1].axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='5% threshold')\n",
    "    axes[1].axhline(y=15, color='red', linestyle='--', alpha=0.7, label='15% threshold')\n",
    "    axes[1].set_xticks(range(len(spec_names)))\n",
    "    axes[1].set_xticklabels([name.replace('_', '\\n').title() for name in spec_names], rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Deviation from Baseline (%)')\n",
    "    axes[1].set_title('Confounder Sensitivity')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    max_conf_deviation = max(deviations)\n",
    "    avg_conf_deviation = np.mean(deviations)\n",
    "    stable_specs = sum([1 for dev in deviations if dev < 10])\n",
    "    \n",
    "    print(f\"\\n📊 CONFOUNDER SENSITIVITY SUMMARY:\")\n",
    "    print(f\"   Maximum deviation: {max_conf_deviation:.1f}%\")\n",
    "    print(f\"   Average deviation: {avg_conf_deviation:.1f}%\")\n",
    "    print(f\"   Stable specifications (<10% deviation): {stable_specs}/{len(deviations)}\")\n",
    "    print(f\"   Sensitivity level: {'✅ Low' if avg_conf_deviation < 8 else '⚠️ Moderate' if avg_conf_deviation < 15 else '❌ High'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Cross-Validation Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation style robustness check\n",
    "print(\"\\n=== CROSS-VALIDATION ROBUSTNESS ===\")\n",
    "print(\"Testing estimate consistency across random data splits...\\n\")\n",
    "\n",
    "n_splits = 8\n",
    "split_results = []\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(n_splits):\n",
    "    # Create random 75% sample\n",
    "    split_df = df.sample(frac=0.75, random_state=42+i)\n",
    "    \n",
    "    try:\n",
    "        dr_result = doubly_robust_estimation(split_df, confounders)\n",
    "        split_results.append({\n",
    "            'split': i+1,\n",
    "            'ate': dr_result['ate'],\n",
    "            'ate_se': dr_result['ate_se'],\n",
    "            'sample_size': len(split_df)\n",
    "        })\n",
    "        print(f\"Split {i+1}: ATE = {dr_result['ate']:.4f} ± {dr_result['ate_se']:.4f} (n={len(split_df):,})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Split {i+1}: Failed - {str(e)[:50]}\")\n",
    "\n",
    "if len(split_results) >= 3:\n",
    "    ate_estimates = [r['ate'] for r in split_results]\n",
    "    \n",
    "    # Calculate consistency metrics\n",
    "    mean_ate = np.mean(ate_estimates)\n",
    "    std_ate = np.std(ate_estimates)\n",
    "    cv_ate = std_ate / abs(mean_ate) if mean_ate != 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n📊 CROSS-VALIDATION SUMMARY:\")\n",
    "    print(f\"   Successful splits: {len(split_results)}/{n_splits}\")\n",
    "    print(f\"   Mean ATE: {mean_ate:.4f}\")\n",
    "    print(f\"   Standard deviation: {std_ate:.4f}\")\n",
    "    print(f\"   Coefficient of variation: {cv_ate:.3f}\")\n",
    "    print(f\"   Range: [{min(ate_estimates):.4f}, {max(ate_estimates):.4f}]\")\n",
    "    print(f\"   Consistency: {'✅ High' if cv_ate < 0.1 else '⚠️ Moderate' if cv_ate < 0.2 else '❌ Low'}\")\n",
    "    \n",
    "    # Visualize cross-validation results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Split-by-split results\n",
    "    split_numbers = [r['split'] for r in split_results]\n",
    "    split_ates = [r['ate'] for r in split_results]\n",
    "    split_errors = [r['ate_se'] for r in split_results]\n",
    "    \n",
    "    axes[0].errorbar(split_numbers, split_ates, yerr=split_errors, \n",
    "                    marker='o', capsize=5, linewidth=2, markersize=8, alpha=0.8)\n",
    "    axes[0].axhline(y=baseline_ate, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Baseline ATE = {baseline_ate:.4f}')\n",
    "    axes[0].axhline(y=mean_ate, color='green', linestyle='--', linewidth=2, \n",
    "                    label=f'CV Mean = {mean_ate:.4f}')\n",
    "    axes[0].fill_between(split_numbers, mean_ate - std_ate, mean_ate + std_ate, \n",
    "                        alpha=0.3, color='green', label=f'±1 SD')\n",
    "    axes[0].set_xlabel('Split Number')\n",
    "    axes[0].set_ylabel('ATE Estimate')\n",
    "    axes[0].set_title('Cross-Validation Results by Split')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of estimates\n",
    "    axes[1].hist(ate_estimates, bins=min(len(ate_estimates), 10), alpha=0.7, color='skyblue', density=True)\n",
    "    axes[1].axvline(mean_ate, color='green', linestyle='-', linewidth=2, label=f'Mean = {mean_ate:.4f}')\n",
    "    axes[1].axvline(baseline_ate, color='red', linestyle='--', linewidth=2, label=f'Baseline = {baseline_ate:.4f}')\n",
    "    axes[1].set_xlabel('ATE Estimate')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].set_title('Distribution of CV Estimates')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Insufficient successful splits for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Method Comparison Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare robustness across different causal methods\n",
    "print(\"\\n=== METHOD COMPARISON ROBUSTNESS ===\")\n",
    "print(\"Comparing estimate stability across different causal inference methods...\\n\")\n",
    "\n",
    "# Test multiple methods on different subsets\n",
    "test_subsets = {\n",
    "    'full_sample': df,\n",
    "    'random_80pct': df.sample(frac=0.8, random_state=42),\n",
    "    'balanced_sample': df.sample(frac=0.7, random_state=123)\n",
    "}\n",
    "\n",
    "method_comparison = {}\n",
    "\n",
    "for subset_name, subset_df in test_subsets.items():\n",
    "    method_comparison[subset_name] = {}\n",
    "    \n",
    "    print(f\"\\n🔍 Testing on {subset_name} (n={len(subset_df):,}):\")\n",
    "    \n",
    "    # Doubly Robust\n",
    "    try:\n",
    "        dr_result = doubly_robust_estimation(subset_df, confounders)\n",
    "        method_comparison[subset_name]['doubly_robust'] = {\n",
    "            'ate': dr_result['ate'],\n",
    "            'ate_se': dr_result['ate_se']\n",
    "        }\n",
    "        print(f\"   ✅ Doubly Robust: {dr_result['ate']:.4f} ± {dr_result['ate_se']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Doubly Robust: Failed\")\n",
    "    \n",
    "    # Propensity Score Matching\n",
    "    try:\n",
    "        matched_data, psm_result = propensity_score_matching(subset_df, confounders, caliper=0.1)\n",
    "        method_comparison[subset_name]['psm'] = {\n",
    "            'ate': psm_result['ate'],\n",
    "            'ate_se': 0  # PSM doesn't provide SE in our implementation\n",
    "        }\n",
    "        print(f\"   ✅ PSM: {psm_result['ate']:.4f} (matched pairs: {psm_result['matched_pairs']})\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ PSM: Failed\")\n",
    "    \n",
    "    # Causal Forest (simplified for speed)\n",
    "    try:\n",
    "        # Use a smaller subset for causal forest to speed up\n",
    "        cf_subset = subset_df.sample(n=min(5000, len(subset_df)), random_state=42) if len(subset_df) > 5000 else subset_df\n",
    "        cf_result = fit_causal_forest(cf_subset, confounders[:10], model_type='standard')  # Use fewer confounders for speed\n",
    "        method_comparison[subset_name]['causal_forest'] = {\n",
    "            'ate': cf_result['ate'],\n",
    "            'ate_se': cf_result['ate_se']\n",
    "        }\n",
    "        print(f\"   ✅ Causal Forest: {cf_result['ate']:.4f} ± {cf_result['ate_se']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Causal Forest: Failed\")\n",
    "\n",
    "# Visualize method comparison\n",
    "methods = ['doubly_robust', 'psm', 'causal_forest']\n",
    "method_labels = ['Doubly Robust', 'PSM', 'Causal Forest']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Method comparison across subsets\n",
    "colors = ['blue', 'red', 'green']\n",
    "width = 0.25\n",
    "\n",
    "subset_names = list(method_comparison.keys())\n",
    "x_pos = np.arange(len(subset_names))\n",
    "\n",
    "for i, (method, label, color) in enumerate(zip(methods, method_labels, colors)):\n",
    "    method_ates = []\n",
    "    for subset_name in subset_names:\n",
    "        if method in method_comparison[subset_name]:\n",
    "            method_ates.append(method_comparison[subset_name][method]['ate'])\n",
    "        else:\n",
    "            method_ates.append(0)  # Failed case\n",
    "    \n",
    "    axes[0].bar(x_pos + i*width, method_ates, width, alpha=0.8, color=color, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Data Subset')\n",
    "axes[0].set_ylabel('ATE Estimate')\n",
    "axes[0].set_title('Method Comparison Across Subsets')\n",
    "axes[0].set_xticks(x_pos + width)\n",
    "axes[0].set_xticklabels([name.replace('_', ' ').title() for name in subset_names])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Method consistency analysis\n",
    "consistency_data = []\n",
    "for method in methods:\n",
    "    method_estimates = []\n",
    "    for subset_name in subset_names:\n",
    "        if method in method_comparison[subset_name]:\n",
    "            method_estimates.append(method_comparison[subset_name][method]['ate'])\n",
    "    \n",
    "    if len(method_estimates) > 1:\n",
    "        consistency = np.std(method_estimates) / abs(np.mean(method_estimates)) if np.mean(method_estimates) != 0 else float('inf')\n",
    "        consistency_data.append(consistency)\n",
    "    else:\n",
    "        consistency_data.append(float('inf'))\n",
    "\n",
    "valid_consistency = [c for c in consistency_data if c != float('inf')]\n",
    "valid_methods = [method_labels[i] for i, c in enumerate(consistency_data) if c != float('inf')]\n",
    "\n",
    "if valid_consistency:\n",
    "    bars = axes[1].bar(valid_methods, valid_consistency, alpha=0.8, color='purple')\n",
    "    axes[1].set_ylabel('Coefficient of Variation')\n",
    "    axes[1].set_title('Method Consistency (Lower = More Robust)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, valid_consistency):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 METHOD ROBUSTNESS SUMMARY:\")\n",
    "for i, (method, label) in enumerate(zip(methods, method_labels)):\n",
    "    if i < len(consistency_data) and consistency_data[i] != float('inf'):\n",
    "        robustness = 'High' if consistency_data[i] < 0.1 else 'Moderate' if consistency_data[i] < 0.2 else 'Low'\n",
    "        print(f\"   {label}: CV = {consistency_data[i]:.3f} ({robustness} robustness)\")\n",
    "    else:\n",
    "        print(f\"   {label}: Insufficient data for assessment\")\n",
    "\n",
    "# Save robustness results\n",
    "robustness_summary = {\n",
    "    'subset_results': subset_results if 'subset_results' in locals() else {},\n",
    "    'confounder_tests': confounder_tests if 'confounder_tests' in locals() else {},\n",
    "    'cv_results': split_results if 'split_results' in locals() else [],\n",
    "    'method_comparison': method_comparison\n",
    "}\n",
    "\n",
    "with open('../results/robustness_results.pkl', 'wb') as f:\n",
    "    pickle.dump(robustness_summary, f)\n",
    "\n",
    "print(\"\\n✅ Robustness analysis complete. Results saved to '../results/robustness_results.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}\n```

**notebooks/07_sensitivity_analysis.ipynb**
```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis for Causal Estimates\n",
    "\n",
    "This notebook conducts sensitivity analysis to test:\n",
    "- Hidden bias (Rosenbaum bounds)\n",
    "- Omitted variable bias\n",
    "- Confounding strength simulation\n",
    "- Placebo tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from src.evaluation.sensitivity_analysis import sensitivity_analysis, rosenbaum_sensitivity_bounds, placebo_tests, omitted_variable_sensitivity\n",
    "from src.models.doubly_robust import doubly_robust_estimation\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data and baseline results\n",
    "df = pd.read_csv('../data/processed/preprocessed_ad_data.csv')\n",
    "confounders = pd.read_csv('../data/processed/confounders.csv')['confounder'].tolist()\n",
    "\n",
    "with open('../results/doubly_robust_results.pkl', 'rb') as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "\n",
    "baseline_ate = baseline_results['ate']\n",
    "print(f\"Baseline ATE: {baseline_ate:.4f}\")\n",
    "print(f\"Dataset: {len(df)} observations\")\n",
    "print(f\"True ATE (simulation): {df['true_effect'].iloc[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Rosenbaum Bounds for Hidden Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to hidden bias using Rosenbaum bounds\n",
    "print(\"=== ROSENBAUM BOUNDS ANALYSIS ===\")\n",
    "print(\"Testing robustness to potential hidden bias...\\n\")\n",
    "\n",
    "# Calculate observed test statistic\n",
    "treated = df[df['treatment'] == 1]['conversion']\n",
    "control = df[df['treatment'] == 0]['conversion']\n",
    "\n",
    "# Wilcoxon rank sum test (Mann-Whitney U)\n",
    "observed_stat, observed_p = stats.mannwhitneyu(treated, control, alternative='two-sided')\n",
    "print(f\"Observed test statistic: {observed_stat}\")\n",
    "print(f\"Observed p-value: {observed_p:.6f}\")\n",
    "print(f\"Effect significant at 5%: {'✅ Yes' if observed_p < 0.05 else '❌ No'}\\n\")\n",
    "\n",
    "# Calculate Rosenbaum bounds for different gamma values\n",
    "gamma_range = np.arange(1.0, 3.1, 0.2)\n",
    "bounds_results = []\n",
    "\n",
    "for gamma in gamma_range:\n",
    "    # Simplified Rosenbaum bounds calculation\n",
    "    # In practice, this would use exact formulas\n",
    "    \n",
    "    n_treated = len(treated)\n",
    "    n_control = len(control)\n",
    "    \n",
    "    # Bias factor from unmeasured confounding\n",
    "    bias_factor = (gamma - 1) / (gamma + 1)\n",
    "    \n",
    "    # Approximate adjustment to test statistic\n",
    "    adjustment = bias_factor * np.sqrt(n_treated * n_control / (n_treated + n_control))\n",
    "    \n",
    "    # Upper and lower bounds for test statistic\n",
    "    adjusted_stat_upper = observed_stat -
